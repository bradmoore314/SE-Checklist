please implement the following enhancement to the existing quoting/configuration web‑app. Add a voice‑first conversational AI agent that lets users speak or type project requirements and automatically create fully specified Access Points, Cameras, Elevators and Intercoms. Use Google Cloud end‑to‑end: streaming Speech‑to‑Text v2 for live transcription, Google Cloud Text‑to‑Speech for audio replies and Gemini 1.5 Flash (Vertex AI, region us‑central1) as the reasoning engine. Expose a WebSocket endpoint /stt that streams microphone audio to Speech‑to‑Text and returns partial and final transcripts with sub‑1.5‑second latency. Send each completed user utterance to POST /agent; there, Gemini Flash must receive the text plus the running project JSON, update or append device objects, determine which required fields are still missing and respond with a deterministic JSON payload containing "reply" (the conversational message) and "project" (the updated configuration object). The front‑end should display the reply, call POST /tts to synthesize it to MP3 using Google Wavenet voices, and autoplay the audio when voice mode is enabled. Provide REST endpoints to create a project, add or update devices and retrieve the full project payload so it can flow into the CPQ layer.

Embed the following system prompt for Gemini Flash: “You are KastleConfig, a project‑scoping assistant. Maintain a JSON object called project. For each device type, ensure all required fields are captured: Access Point → name, floor, location_desc, door_type, reader_type, power_source; Camera → name, floor, mount_type, coverage_area, resolution, stream_count; Elevator → bank_id, num_cabs, floors_served, reader_type, controller_type; Intercom → location, unit_type, call_targets, network_type. Whenever the user describes a new device, create or append it; for every missing required field, ask exactly one targeted follow‑up question; once a device is complete, confirm its summary. Always return valid JSON with keys reply and project.”

Front‑end requirements: a mic button that opens the /stt stream, shows partial transcripts, sends final text to /agent, renders the agent’s reply bubble, plays the TTS response if voice mode is toggled on and continuously mirrors the evolving project JSON in a table or card view. Maintain the corporate white background with #FF0000 trim.

Back‑end stack: Python 3.11, FastAPI, uvicorn, google‑cloud‑speech, google‑cloud‑texttospeech, google‑cloud‑aiplatform, websockets, and pydantic. Authenticate via a single Google service‑account JSON mounted as a Replit secret (GOOGLE_APPLICATION_CREDENTIALS=/secrets/gcp.json, plus GCP_PROJECT_ID and GCP_REGION). Expose port 7860 (or the default Replit port). Do not commit credentials—use the Replit secret manager.

Acceptance criteria: streaming transcription feels instantaneous; TTS returns natural audio; /agent always yields syntactically valid JSON and the agent reliably fills all required fields; project JSON persists and is retrievable; README documents Google API enablement, service‑account creation, environment variables and local run steps.

Before coding, confirm: the daily Speech and TTS quota limits; whether MP3 or OGG_OPUS is preferred for returned audio; details of the target database/CPQ schema; whether the agent should support editing previously captured devices; and any additional UX or branding constraints beyond the white background and red trim.